 #UCSC #CSE144 
 
##### Readings 6.1-6.2

**Limitation of Fixed Basis Function**
We expect that a sufficiently large and rich set of *basis functions*. There are shortcomings which arise from that the *basis function* $\phi_j(x)$ are fixed and independent of the training data

**6.1: The curse of dimensionality**
The difficulty with both polynomial  regression and Iris data classification arise as the basis function was chosen independently of the problem. As we increase the number of inputs, lets say $D$ inputs, then the general polynomial with coefficients is up to order 3
$$y(x,\omega) = \omega_0 \sum_{i=1}^{D} \omega_i x_i + \sum_{j=1}^{D} \sum_{i=1}^{D}\omega_{ij}x_ix_j+\sum_{i=1}^{D}\sum_{j=1}^{D}\sum_{k=1}^{D}\omega_{ijk}x_ix_jx_k$$ 
**High-dimensional spaces**
In higher dimensions, most of the "volume" is concentrated near the surface
$$\frac{V_D(1) - V_D(1-\epsilon)}{V_D(1)} = 1-(1-\epsilon)^D$$
Showing for large dimensions $D$, even a small value of $\epsilon$ means most volume is concentrated near the outer shell. Much of the data lies near the surface which leads to this *curse of dimensionality*

- High dimensional spaces concentrate the volume near the surfaces, *data* near the surface

**Data Manifolds**
In high dimensional spaces, real data occupies a smaller subset or *manifold*. Rather than filling the entire space, the manifold is a lower dimensional space embedded into the higher space.

The problem with a fixed basis function is that they don't adapt to the data. Normally we need a basis function that adapts to the structure of the data and finds patterns in higher dimensional spaces.

**Data-dependent basis functions**
A better approach as apposed is a data driven method to determine the basis function. The basis function essentially is adapted based on the training data, allowing it to learn different features from the data

An example being *radial basis functions* where a separate basis functions centered on each training data point
$$\phi_n(x)=exp(-\frac{||x-x_n||^2}{s^2})$$
Where $x_n$ is a training point, and $s$ controls the width of the basis function. The idea is to create a smoother function adapting to the underlying manifold of the data

**6.2: Multilayer Networks**
The key idea behind neural networks is to choose basis functions $\phi_j(x)$ that themselves have learnable parameters, then allowing these parameters to be adjusted, along with the coefficients $w_j$ during training. You can then optimize the model by minimizing an error function utilizing methods like stochastic gradient descent
One of the most successful choices in creating these nonlinear basis functions, is to allow each basis function to be a linear combination of the inputs, where coefficients in the linear combination are learnable parameters. Considering a basic neural network model having two layers of learnable parameters, you construct $M$ linear combinations of the input variables, $x_1,...,x_D$ in the form
$$a_j^{(1)} = \sum_{i=1}^Dw_{ji}^{(1)}x_i + w_{j0}^{(1)}$$
where $j=1,..,M$ and the superscript $(1)$ indicates the corresponding parameters are in the first 'layer' of the network. The outputs are computed similarly by summing the transformed hidden units for 'layer 2'
$$a_k^{(k)} = \sum_{j=1}^Mw+{kj}^{(2)}z_j^{(1)}+w_{k0}^{(2)}$$
Finally the $a_k^{(2)}$ are transformed using the appropriate *output activation function* to give the output $y_k$

**Parameter Matrices**
Bias parameters can be absorbed into the set of weight parameters by defining an additional input variable $x_0$ whose value is clamped at $x_0 = 1$, taking the form 
$$a_j = \sum_{i=0}^Dw_{ji}^{(1)}x_i$$
Similar you can absorb the second layer into a basis into a second layer weight. Another notation in this book to represent $y_k(x,w) = f(W^{(2)}h(W^{(1)}c))$ here the weight and bias terms are included in the matrices 

**Universal Approximation**
The *universal approximation theorem* states that a two layer network can approximate any continuous function to some arbitrary accuracy, given a sufficient amount of hidden units. Although more complex than two layer networks will prove later to be more practical and learn more hierarchical internal representations.

**Hidden Unit Activation Functions**
The activation functions introduce non-linearity into the model allowing for models to capture more complex patterns
- Types of Activations
	- Sigmoid $\sigma(a) = \frac{1}{1+exp(-a)}$
		- often used but suffers from vanishing gradients for larger inputs
	- Tanh: $tanh(a) = \frac{e^a-e^{-a}}{e^a+e^{-a}}$ 
		- similar to sigmoid but outputs $[-1,1]$
	- ReLU: $ReLU(a) = max(0,a)$ 
		- used to its simplicity but issues with dead neurons
	- Leaky ReLU: $Leaky$ $ReLU(a) = max(0,a) + \alpha min(0,a)$ 
		- variant of ReLU that allows small gradient for negative inputs
- These activation functions introduce nonlinear transformations

**Weight-space symmetries**
Neural networks have *weight-space symmetries* which means that multiple distinct weight configurations can result in the same output for any given input. These symmetries normally don't affect learning but show there may be equivalent solutions within the optimization process.

##### Readings 6.3.1-6.3.3

**Hierarchical Representations**
We see in deep neural networks that the network architecture encodes the input space through a hierarchical representation. When coming across the task of recognizing objects in images, the relationship between pixels of and image and the high-level concept of an object like a cat is complex and non-linear. A deep neural network can learn to detect low level features within early layers. This can be viewed as a *compositional* inductive bias, in which a higher level object can be detected utilizing lower level objects (i.e edges, eyes, whiskers, and other features)

**Distributed Representations**
A neural network can take advantage of another form of compositionality called *distributed representation*. Each hidden layer an be thought of as representing a feature, a high value of activation indicates its presence. However, the network could potentially learn a different representation in which *combinations* of hidden units represent features, potentially a hidden layer to represent $2^M$ features, growing exponentially with the number of units. An example is a network designed to process images of faces, as each face may or may not have glasses/hat/beard, leading to eight combinations of the same face. This can be represented with eight units each of which turns on, this can also be represented more compactly with just three units, one for each attribute. 

**Representation Learning**
With a deep neural network, we can view successive layers as preforming transformations of data (i.e classify skin lesions: benign or malignant) The ability to discover nonlinear transformations of the data is called *representation learning*. Sometimes called the *embedding space* is given by the outputs of one of the hidden layers of the network, so that any input vector can be transformed into this representation by forward propagation through the network. 

Representation learning can be powerful as it allows us to exploit unlabeled data. Learning from unlabeled data is called *unsupervised learning*. Networks known as *autoencoders* undergo training that will force the network to discover some internal representation for the data that is useful for solving tasks (i.e image classification)